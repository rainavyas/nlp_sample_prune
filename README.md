# Objective

Training with state of the art Transformer-based networks has a computational cost of O(n^2), where n is the length of the sequence. In this work, we explore reducing the the average sequence length, n, to speed up training time, without compromising model performance.
